{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7039247b-ed49-4cc0-8d29-389a1e04eb05",
   "metadata": {},
   "source": [
    "Test of  Mask Graph N-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bbf0d2-5279-46ce-9e54-5edbfacf7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e685db6-fd5d-4d8f-b4d6-fdb8486bb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedLinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias, mask is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            # change weight to 0 where mask == 0  \n",
    "            for i in range(mask.shape[0]):\n",
    "                for j in range(mask.shape[1]):\n",
    "                    if torch.equal(mask[i][j], torch.tensor(2.)):\n",
    "                        weight[i][j]=1\n",
    "                    else:\n",
    "                        weight[i][j] = weight[i][j] * mask[i][j]\n",
    "                \n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, mask = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = grad_mask = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            if mask is not None:\n",
    "                # change grad_weight to 0 where mask == 0\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(mask.shape[1]):\n",
    "                        if torch.equal(mask[i][j], torch.tensor(2.)):\n",
    "                            grad_weight[i][j]=0\n",
    "                        else:\n",
    "                            grad_weight[i][j] = grad_weight[i][j] * mask[i][j]\n",
    "        #if bias is not None and ctx.needs_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, grad_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e526f2c4-0132-4877-9ae6-b1f21af2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self, mask, bias=True):\n",
    "        \"\"\"\n",
    "        extended torch.nn module which mask connection.\n",
    "        Argumens\n",
    "        ------------------\n",
    "        mask [torch.tensor]:\n",
    "            the shape is (n_input_feature, n_output_feature).\n",
    "            the elements are 0 or 1 which declare un-connected or\n",
    "            connected.\n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear, self).__init__()\n",
    "        self.input_features = mask.shape[0]\n",
    "        self.output_features = mask.shape[1]\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            self.mask = mask.type(torch.float).t()\n",
    "        else:\n",
    "            self.mask = torch.tensor(mask, dtype=torch.float).t()\n",
    "\n",
    "        self.mask = nn.Parameter(self.mask, requires_grad=False)\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.output_features, self.input_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(self.output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # mask weight\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return CustomizedLinearFunction.apply(input, self.weight, self.bias, self.mask)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e35170c-591f-4f02-89ec-ca214a2377bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == 'check grad':\n",
    "    from torch.autograd import gradcheck\n",
    "\n",
    "    # gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "    # evaluated with these tensors are close enough to numerical\n",
    "    # approximations and returns True if they all verify this condition.\n",
    "\n",
    "    customlinear = CustomizedLinearFunction.apply\n",
    "\n",
    "    input = (\n",
    "            torch.randn(20,20,dtype=torch.double,requires_grad=True),\n",
    "            torch.randn(30,20,dtype=torch.double,requires_grad=True),\n",
    "            None,\n",
    "            None,\n",
    "            )\n",
    "    test = gradcheck(customlinear, input, eps=1e-6, atol=1e-4)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da948bc-484f-48d5-942a-c6eaaea28bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mask matrix to customize linear\n",
    "mask = torch.tensor(\n",
    "  [[1, 2, 1],\n",
    "   [0, 1, 0],\n",
    "   [1, 2, 1],\n",
    "   [1, 0, 1],]\n",
    "  )\n",
    "\n",
    "# define size of layers.\n",
    "# this architect is [INPUT, HIDDEN(masked(customized) linear), OUTPUT]-layers.\n",
    "Dim_INPUT  = mask.shape[0]\n",
    "Dim_HIDDEN = mask.shape[1]\n",
    "Dim_OUTPUT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb73de1-7194-4857-bf07-56b2d64dada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create randomly input:x, output:y as train dataset.\n",
    "batch = 1\n",
    "x = torch.randn(batch, Dim_INPUT)\n",
    "y = torch.randn(batch, Dim_OUTPUT)\n",
    "\n",
    "# pipe as model\n",
    "model = torch.nn.Sequential(\n",
    "        CustomizedLinear(mask, bias=None), # dimmentions is set from mask.size \n",
    "        torch.nn.Linear(Dim_HIDDEN, Dim_OUTPUT, bias=None),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc619aff-0537-4768-9a91-9fd627ff5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== mask matrix ===\n",
      "tensor([[1, 2, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 2, 1],\n",
      "        [1, 0, 1]])\n",
      "===================\n",
      "--- epoch=0, loss=1.7676911354064941 ---\n",
      "↓↓↓masked weight↓↓↓\n",
      "tensor([[-0.1822,  1.0000, -0.0035],\n",
      "        [ 0.0000, -0.1386,  0.0000],\n",
      "        [-0.3157,  1.0000,  0.3445],\n",
      "        [ 0.0891, -0.0000,  0.1516]], requires_grad=True)\n",
      "↓↓↓masked grad of weight↓↓↓\n",
      "tensor([[ 0.0485,  0.0000, -0.2757],\n",
      "        [ 0.0000, -0.3177, -0.0000],\n",
      "        [-0.1935,  0.0000,  1.1005],\n",
      "        [-0.0396,  0.0000,  0.2250]])\n",
      "--- epoch=1, loss=1.0690386295318604 ---\n",
      "↓↓↓masked weight↓↓↓\n",
      "tensor([[-0.1828,  1.0000,  0.0165],\n",
      "        [ 0.0000, -0.1244,  0.0000],\n",
      "        [-0.3130,  1.0000,  0.2648],\n",
      "        [ 0.0897, -0.0000,  0.1353]], requires_grad=True)\n",
      "↓↓↓masked grad of weight↓↓↓\n",
      "tensor([[ 0.0067,  0.0000, -0.1997],\n",
      "        [ 0.0000, -0.1423,  0.0000],\n",
      "        [-0.0269,  0.0000,  0.7970],\n",
      "        [-0.0055,  0.0000,  0.1629]])\n",
      "--- epoch=2, loss=0.5059181451797485 ---\n",
      "↓↓↓masked weight↓↓↓\n",
      "tensor([[-0.1797,  1.0000,  0.0307],\n",
      "        [ 0.0000, -0.1274,  0.0000],\n",
      "        [-0.3257,  1.0000,  0.2080],\n",
      "        [ 0.0871, -0.0000,  0.1237]], requires_grad=True)\n",
      "↓↓↓masked grad of weight↓↓↓\n",
      "tensor([[-0.0317,  0.0000, -0.1422],\n",
      "        [ 0.0000,  0.0307,  0.0000],\n",
      "        [ 0.1266,  0.0000,  0.5675],\n",
      "        [ 0.0259,  0.0000,  0.1160]])\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "print('=== mask matrix ===')\n",
    "print(mask)\n",
    "print('===================')\n",
    "learning_rate = 0.1\n",
    "for t in range(3):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # loss\n",
    "    loss = (y_pred - y).abs().mean()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Use autograd to compute the backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            # mask is also saved in param, but mask.requires_grad=False\n",
    "            if param.requires_grad: \n",
    "                param -= learning_rate * param.grad\n",
    "                # check masked param.grad\n",
    "                if np.array(param.grad).size == np.array(mask).size:\n",
    "                    print('--- epoch={}, loss={} ---'.format(t,loss.item()))\n",
    "                    print('↓↓↓masked weight↓↓↓')\n",
    "                    print(param.t())\n",
    "                    print('↓↓↓masked grad of weight↓↓↓')\n",
    "                    print(param.grad.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902b651-0599-4754-b4a1-6c5680f4e016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
